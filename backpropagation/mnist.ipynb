{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №1. Backward-Propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнил: **Кирилл Петров. ВМиСТ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Propagation\n",
    "\n",
    "Построим сеть следующего вида:\n",
    "\n",
    "$$\n",
    "X \\rightarrow Z=W_1X \\rightarrow U = W_2Z \\rightarrow S=F_{softmax}(U) \\rightarrow L(S, y) = -\\log S_y,\n",
    "$$\n",
    "\n",
    "где $S_y=\\frac{\\exp(U_y)}{\\sum_{j=0}^{K-1}\\exp(U_j)}$; y-ый элемент $S$ и $U_y$ y-ый элемент $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-Propagation\n",
    "\n",
    "Посчитаем градиенты $\\frac{\\partial L}{\\partial W_2}$ и $\\frac{\\partial L}{\\partial W_1}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial U_t} =  \n",
    "\\begin{cases}\n",
    "S_t(U), & t\\neq y \\\\\n",
    "1- S_t(U). & t = y\n",
    "\\end{cases}\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial L}{\\partial U} = e_y - S(U), \n",
    "$$\n",
    "\n",
    "где $e_y$ единичный вектор, где y-ый равный 1, если y, в противном случае 0. \n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial U}\\frac{\\partial U}{\\partial W_1} = (e_y - S(U))Z^T \\\\\n",
    "& \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial U}\\frac{\\partial U}{\\partial Z}\\frac{\\partial Z}{\\partial W_1} = \\big(\\big(e_y - S(U)\\big) \\cdot W_2\\big)X^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соорудим и реализуем небольшой класс, который реализует описанные выше действия с градиентным методом **Adagrad**:\n",
    "\n",
    "Обозначим градиент по параметру $W_i$ на итерации $t$ как $g_{t,i} = \\nabla_{W}L(W_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $W_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ W_{t+1, i} = W_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ W_{t+1, i} = W_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $W_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ W_{t+1} = W_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализацию весов проводил по правилу xavier (брал из этой [статьи](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)) : $ Uniform(-a; a) $\n",
    "Где\n",
    "$$\n",
    "a = \\frac{\\sqrt 6}{\\sqrt{n_i + n_{i+1}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(100), solver='adagrad',\n",
    "                 batch_size=1, learning_rate=0.001, momentum=0.9, eps=1e-8,\n",
    "                 max_iter=200, random_state=32, shuffle=True, verbose=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __stable_softmax(self, x):\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        return x / x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def __crossentropy_loss(self, y_true, y_prob):\n",
    "        return - xlogy(y_true, y_prob).sum()\n",
    "\n",
    "    def __forward_layer(self, x, w, activation_function):\n",
    "        out = np.dot(x, w)\n",
    "        if activation_function is not None:\n",
    "            out = activation_function(out)\n",
    "        return out\n",
    "\n",
    "    def __forward_propagate(self, x):\n",
    "        weights = self.weights\n",
    "        out_activations = [x]\n",
    "        for weight, activataion in zip(weights, self.functions):\n",
    "            out = self.__forward_layer(out_activations[-1], weight, activataion)\n",
    "            out_activations.append(out)\n",
    "        return out_activations\n",
    "\n",
    "    def __back_propagation(self, activations, y):\n",
    "        weights = self.weights\n",
    "        coef_grads = [np.empty_like(a_layer) for a_layer in weights]\n",
    "\n",
    "        deltas = activations[-1] - y\n",
    "        coef_grads[-1] = np.dot(activations[-2].T, deltas)\n",
    "\n",
    "        for i in range(len(weights)-2, -1, -1):\n",
    "            deltas =  np.dot(deltas, weights[i + 1].T)\n",
    "            coef_grads[i] = np.dot(activations[i].T, deltas)\n",
    "\n",
    "        return coef_grads\n",
    "\n",
    "    def __init_layer(self, input_size, output_size):\n",
    "        a = np.sqrt(6.0 / (input_size + output_size))\n",
    "        w = np.random.uniform(-a, a, (input_size, output_size))\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self._label_binarizer = LabelBinarizer()\n",
    "        y_train = y\n",
    "        X_train = X\n",
    "        y = self._label_binarizer.fit_transform(y)\n",
    "        self._num_classes = len(self._label_binarizer.classes_)\n",
    "\n",
    "        n, p = X.shape\n",
    "        s = self.hidden_layer_sizes[0]\n",
    "\n",
    "        self.weights = [\n",
    "            self.__init_layer(p, s),\n",
    "            self.__init_layer(s, self._num_classes)\n",
    "        ]\n",
    "\n",
    "        self.functions = [\n",
    "            None,\n",
    "            self.__stable_softmax,\n",
    "        ]\n",
    "\n",
    "        accum_grad = [np.zeros_like(param) for param in self.weights]\n",
    "\n",
    "        for j in range(self.max_iter):\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "            if self.shuffle:\n",
    "                indices = np.arange(n)\n",
    "                np.random.shuffle(indices)\n",
    "                X = X.take(indices, axis=0)\n",
    "                y = y.take(indices, axis=0)\n",
    "            \n",
    "            for i in range(0, n, self.batch_size):\n",
    "                X_batch = X[i : i + self.batch_size]\n",
    "                y_batch = y[i : i + self.batch_size]\n",
    "\n",
    "                activations = self.__forward_propagate(X_batch)\n",
    "\n",
    "                y_prob = activations[-1]\n",
    "\n",
    "                accumulated_loss += self.__crossentropy_loss(y_batch, y_prob)\n",
    "                coef_grads = self.__back_propagation(activations, y_batch)\n",
    "\n",
    "                coef_grads = [grad / self.batch_size for grad in coef_grads]\n",
    "                accum_grad = [accum + grad**2 for accum, grad in zip(accum_grad, coef_grads)]\n",
    "                inv_accum_grad = [self.learning_rate / np.sqrt(self.eps + accum) for accum in accum_grad]\n",
    "                self.weights = [weight - inv_accum * grad for weight, inv_accum, grad in zip(self.weights, inv_accum_grad, coef_grads)]\n",
    "\n",
    "            if self.verbose:\n",
    "                loss = accumulated_loss / X.shape[0]\n",
    "                y_pred = self.predict(X_train)\n",
    "                accuracy = (y_pred == y_train).mean()\n",
    "                print(\"Epoch {}/{};\\t Train accuracy: {:.3f} \\t Loss : {:.3f}\".format(j + 1, self.max_iter, accuracy, loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self.__forward_propagate(X)\n",
    "        y_pred = activations[-1]\n",
    "        return self._label_binarizer.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (60000, 784) (60000,)\n",
      "test size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "data_train = pd.read_csv(\"../dataset/mldata/mnist_train.csv\", header=None)\n",
    "data_test = pd.read_csv(\"../dataset/mldata/mnist_test.csv\", header=None)\n",
    "\n",
    "x_train = np.ascontiguousarray(data_train[data_train.columns[:-1]].values, dtype=np.float32)\n",
    "y_train = np.ascontiguousarray(data_train[data_train.columns[-1]].values, dtype=np.float32)\n",
    "x_test = np.ascontiguousarray(data_test[data_test.columns[:-1]].values, dtype=np.float32)\n",
    "y_test = np.ascontiguousarray(data_test[data_test.columns[-1]].values, dtype=np.float32)\n",
    "\n",
    "print('train size: ', x_train.shape, y_train.shape)\n",
    "print('test size: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы веса не улетали в большие значения, нормируем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10;\t Train accuracy: 0.918 \t Loss : 0.626\n",
      "Epoch 2/10;\t Train accuracy: 0.925 \t Loss : 0.293\n",
      "Epoch 3/10;\t Train accuracy: 0.927 \t Loss : 0.278\n",
      "Epoch 4/10;\t Train accuracy: 0.927 \t Loss : 0.272\n",
      "Epoch 5/10;\t Train accuracy: 0.930 \t Loss : 0.267\n",
      "Epoch 6/10;\t Train accuracy: 0.931 \t Loss : 0.263\n",
      "Epoch 7/10;\t Train accuracy: 0.930 \t Loss : 0.260\n",
      "Epoch 8/10;\t Train accuracy: 0.931 \t Loss : 0.257\n",
      "Epoch 9/10;\t Train accuracy: 0.931 \t Loss : 0.256\n",
      "Epoch 10/10;\t Train accuracy: 0.932 \t Loss : 0.254\n",
      "\n",
      "Accuracy на тестовой выборке:  0.9253\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = DNNClassifier(hidden_layer_sizes=(100,), max_iter=10, solver='adagrad',\n",
    "     batch_size=256, learning_rate=0.05, random_state=777, shuffle=True, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print()\n",
    "print('Accuracy на тестовой выборке: ', (y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну видно, что сеть работает и при том, хорошо - **Loss уменьшается**, **accuracy** на тренировочной выборке **растет**. Да и **работает** довольно **быстро**. Я доволен :) Осталось подтюнить гиперпараметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameters for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, нашей сетью, попробуем подобрать оптимальные гиперпараметры по размеру скрытого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший подбор параметра для DNNClassifier: {'hidden_layer_sizes': (16,)}\n",
      "Лучший scope для DNNClassifier: 0.92155\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'hidden_layer_sizes': [(16,), (32,), (48,), (64,), (76,), (92,)],\n",
    "}\n",
    "\n",
    "estimator = DNNClassifier(solver='adagrad',\n",
    "     batch_size=256, learning_rate=0.05, max_iter=50,\n",
    "     random_state=777, verbose=False)\n",
    "\n",
    "grid_clf = GridSearchCV(estimator, parameters, cv=5, scoring='accuracy')\n",
    "grid_clf.fit(x_train, y_train)\n",
    "print(\"Лучший подбор параметра для DNNClassifier: {}\".format(grid_clf.best_params_))\n",
    "print(\"Лучший scope для DNNClassifier: {}\".format(grid_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEBCAYAAABmCeILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB6o+Lwwcm9nYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(grid_clf.cv_results_).plot(x='param_hidden_layer_sizes', y=[ 'mean_test_score', 'mean_train_score'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что при большом числе нейронов в скрытом слое мы наблюдаем **переобучение**. Возьмем количетсво нейронов, в самом оптимальном варианте (16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(16,),\n",
       "       learning_rate=0.05, max_iter=30, momentum=0.9, random_state=777,\n",
       "       shuffle=True, solver=None, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_clf.best_estimator_\n",
    "up_params = {'verbose': True, 'max_iter': 30}\n",
    "best_estimator.set_params(**up_params)\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30;\t Train accuracy: 0.914 \t Loss : 0.448\n",
      "Epoch 2/30;\t Train accuracy: 0.921 \t Loss : 0.299\n",
      "Epoch 3/30;\t Train accuracy: 0.923 \t Loss : 0.283\n",
      "Epoch 4/30;\t Train accuracy: 0.926 \t Loss : 0.275\n",
      "Epoch 5/30;\t Train accuracy: 0.928 \t Loss : 0.270\n",
      "Epoch 6/30;\t Train accuracy: 0.929 \t Loss : 0.266\n",
      "Epoch 7/30;\t Train accuracy: 0.929 \t Loss : 0.263\n",
      "Epoch 8/30;\t Train accuracy: 0.930 \t Loss : 0.261\n",
      "Epoch 9/30;\t Train accuracy: 0.931 \t Loss : 0.259\n",
      "Epoch 10/30;\t Train accuracy: 0.929 \t Loss : 0.257\n",
      "Epoch 11/30;\t Train accuracy: 0.932 \t Loss : 0.256\n",
      "Epoch 12/30;\t Train accuracy: 0.932 \t Loss : 0.254\n",
      "Epoch 13/30;\t Train accuracy: 0.932 \t Loss : 0.253\n",
      "Epoch 14/30;\t Train accuracy: 0.933 \t Loss : 0.252\n",
      "Epoch 15/30;\t Train accuracy: 0.933 \t Loss : 0.251\n",
      "Epoch 16/30;\t Train accuracy: 0.933 \t Loss : 0.250\n",
      "Epoch 17/30;\t Train accuracy: 0.933 \t Loss : 0.249\n",
      "Epoch 18/30;\t Train accuracy: 0.934 \t Loss : 0.249\n",
      "Epoch 19/30;\t Train accuracy: 0.933 \t Loss : 0.248\n",
      "Epoch 20/30;\t Train accuracy: 0.934 \t Loss : 0.247\n",
      "Epoch 21/30;\t Train accuracy: 0.934 \t Loss : 0.247\n",
      "Epoch 22/30;\t Train accuracy: 0.934 \t Loss : 0.246\n",
      "Epoch 23/30;\t Train accuracy: 0.934 \t Loss : 0.246\n",
      "Epoch 24/30;\t Train accuracy: 0.934 \t Loss : 0.245\n",
      "Epoch 25/30;\t Train accuracy: 0.934 \t Loss : 0.245\n",
      "Epoch 26/30;\t Train accuracy: 0.934 \t Loss : 0.244\n",
      "Epoch 27/30;\t Train accuracy: 0.935 \t Loss : 0.244\n",
      "Epoch 28/30;\t Train accuracy: 0.934 \t Loss : 0.243\n",
      "Epoch 29/30;\t Train accuracy: 0.935 \t Loss : 0.243\n",
      "Epoch 30/30;\t Train accuracy: 0.935 \t Loss : 0.243\n",
      "Wall time: 49.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(16,),\n",
       "       learning_rate=0.05, max_iter=30, momentum=0.9, random_state=777,\n",
       "       shuffle=True, solver=None, verbose=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "best_estimator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и финальное аккураси:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тестовой выборке:  0.926\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_estimator.predict(x_test)\n",
    "print('Accuracy на тестовой выборке: ', (y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus the sklearn MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним нашу сеть, с какой нибудь библиотекой. Для простоты пускай это будет *sklearn*. И подбрем аналогично нашей сети параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73436694\n",
      "Iteration 2, loss = 0.35062043\n",
      "Iteration 3, loss = 0.30739383\n",
      "Iteration 4, loss = 0.28902413\n",
      "Iteration 5, loss = 0.27904313\n",
      "Iteration 6, loss = 0.27252593\n",
      "Iteration 7, loss = 0.26722828\n",
      "Iteration 8, loss = 0.26373192\n",
      "Iteration 9, loss = 0.26013310\n",
      "Iteration 10, loss = 0.25787084\n",
      "Iteration 11, loss = 0.25518767\n",
      "Iteration 12, loss = 0.25345777\n",
      "Iteration 13, loss = 0.25129781\n",
      "Iteration 14, loss = 0.25017786\n",
      "Iteration 15, loss = 0.24874089\n",
      "Iteration 16, loss = 0.24708613\n",
      "Iteration 17, loss = 0.24613000\n",
      "Iteration 18, loss = 0.24500568\n",
      "Iteration 19, loss = 0.24473631\n",
      "Iteration 20, loss = 0.24350242\n",
      "Iteration 21, loss = 0.24221757\n",
      "Iteration 22, loss = 0.24145957\n",
      "Iteration 23, loss = 0.24186943\n",
      "Iteration 24, loss = 0.24089721\n",
      "Iteration 25, loss = 0.23973297\n",
      "Iteration 26, loss = 0.23929363\n",
      "Iteration 27, loss = 0.23851964\n",
      "Iteration 28, loss = 0.23824422\n",
      "Iteration 29, loss = 0.23767271\n",
      "Iteration 30, loss = 0.23757961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=0, batch_size=256, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(16,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=777, shuffle=True, solver='adam', tol=0,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,), max_iter=30, alpha=0, activation='identity',\n",
    "                     batch_size=256, solver='adam', verbose=1, random_state=777,\n",
    "                     tol=0, shuffle=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на тестовой выборке:  0.9267\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy на тестовой выборке: ', (y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти аналогичный результат, схожей с моей моделью. Лучше аккураси получается при **нелинейной** функции активации после скрытого слоя, например ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81596686\n",
      "Iteration 2, loss = 0.35609414\n",
      "Iteration 3, loss = 0.29645522\n",
      "Iteration 4, loss = 0.26542929\n",
      "Iteration 5, loss = 0.24571562\n",
      "Iteration 6, loss = 0.23178049\n",
      "Iteration 7, loss = 0.22053985\n",
      "Iteration 8, loss = 0.21187050\n",
      "Iteration 9, loss = 0.20409084\n",
      "Iteration 10, loss = 0.19801073\n",
      "Iteration 11, loss = 0.19210298\n",
      "Iteration 12, loss = 0.18623756\n",
      "Iteration 13, loss = 0.17910247\n",
      "Iteration 14, loss = 0.17381358\n",
      "Iteration 15, loss = 0.16914523\n",
      "Iteration 16, loss = 0.16547316\n",
      "Iteration 17, loss = 0.16175265\n",
      "Iteration 18, loss = 0.15866246\n",
      "Iteration 19, loss = 0.15524834\n",
      "Iteration 20, loss = 0.15315301\n",
      "Iteration 21, loss = 0.15055781\n",
      "Iteration 22, loss = 0.14809033\n",
      "Iteration 23, loss = 0.14636946\n",
      "Iteration 24, loss = 0.14425310\n",
      "Iteration 25, loss = 0.14191990\n",
      "Iteration 26, loss = 0.14030226\n",
      "Iteration 27, loss = 0.13819602\n",
      "Iteration 28, loss = 0.13657976\n",
      "Iteration 29, loss = 0.13528631\n",
      "Iteration 30, loss = 0.13427411\n",
      "Accuracy на тестовой выборке:  0.9502\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(16,), max_iter=30, alpha=0, activation='relu',\n",
    "                     batch_size=256, solver='adam', verbose=10, random_state=777,\n",
    "                     tol=0, shuffle=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy на тестовой выборке: ', (y_pred == y_test).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

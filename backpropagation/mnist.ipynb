{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Propagation\n",
    "\n",
    "$$\n",
    "X \\rightarrow Z=W_1X \\rightarrow U = W_2Z \\rightarrow S=F_{softmax}(U) \\rightarrow L(S, y) = -\\log S_y,\n",
    "$$\n",
    "\n",
    "where $S_y=\\frac{\\exp(U_y)}{\\sum_{j=0}^{K-1}\\exp(U_j)}$ is the y-th element of the $S$ and $U_y$ is the y-th element of the $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-Propagation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial U_t} =  \n",
    "\\begin{cases}\n",
    "S_t(U), & t\\neq y \\\\\n",
    "1- S_t(U). & t = y\n",
    "\\end{cases}\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial L}{\\partial U} = e_y - S(U), \n",
    "$$\n",
    "\n",
    "where $e_y$ is the unit vector, which y-th coordinate equals to 1 and 0 elsewhere. \n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial U}\\frac{\\partial U}{\\partial W_1} = (e_y - S(U))Z^T \\\\\n",
    "& \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial U}\\frac{\\partial U}{\\partial Z}\\frac{\\partial Z}{\\partial W_1} = \\big(\\big(e_y - S(U)\\big) \\cdot W_2\\big)X^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(100), solver='adagrad',\n",
    "                 batch_size=1, learning_rate=0.001, momentum=0.9, eps=1e-8,\n",
    "                 max_iter=200, random_state=32, shuffle=True, verbose=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __stable_softmax(self, x):\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        return x / x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def __crossentropy_loss(self, y_true, y_prob):\n",
    "        return - xlogy(y_true, y_prob).sum()\n",
    "\n",
    "    def __forward_layer(self, x, w, activation_function):\n",
    "        out = np.dot(x, w)\n",
    "        if activation_function is not None:\n",
    "            out = activation_function(out)\n",
    "        return out\n",
    "\n",
    "    def __forward_propagate(self, x):\n",
    "        weights = self.weights\n",
    "        out_activations = [x]\n",
    "        for weight, activataion in zip(weights, self.functions):\n",
    "            out = self.__forward_layer(out_activations[-1], weight, activataion)\n",
    "            out_activations.append(out)\n",
    "        return out_activations\n",
    "\n",
    "    def __back_propagation(self, activations, y):\n",
    "        weights = self.weights\n",
    "        coef_grads = [np.empty_like(a_layer) for a_layer in weights]\n",
    "\n",
    "        deltas = activations[-1] - y\n",
    "        coef_grads[-1] = np.dot(activations[-2].T, deltas)\n",
    "\n",
    "        for i in range(len(weights)-2, -1, -1):\n",
    "            deltas =  np.dot(deltas, weights[i + 1].T)\n",
    "            coef_grads[i] = np.dot(activations[i].T, deltas)\n",
    "\n",
    "        return coef_grads\n",
    "\n",
    "    def __init_layer(self, input_size, output_size):\n",
    "        a = 2.0/(input_size + output_size)\n",
    "        w = np.random.uniform(-a, a, (input_size, output_size))\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self._label_binarizer = LabelBinarizer()\n",
    "        y_train = y\n",
    "        X_train = X\n",
    "        y = self._label_binarizer.fit_transform(y)\n",
    "        self._num_classes = len(self._label_binarizer.classes_)\n",
    "\n",
    "        n, p = X.shape\n",
    "        s = self.hidden_layer_sizes[0]\n",
    "\n",
    "        self.weights = [\n",
    "            self.__init_layer(p, s),\n",
    "            self.__init_layer(s, self._num_classes)\n",
    "        ]\n",
    "\n",
    "        self.functions = [\n",
    "            None,\n",
    "            self.__stable_softmax,\n",
    "        ]\n",
    "\n",
    "        accum_grad = [np.zeros_like(param) for param in self.weights]\n",
    "\n",
    "        for j in range(self.max_iter):\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "            if self.shuffle:\n",
    "                indices = np.arange(n)\n",
    "                np.random.shuffle(indices)\n",
    "                X = X.take(indices, axis=0)\n",
    "                y = y.take(indices, axis=0)\n",
    "            \n",
    "            for i in range(0, n, self.batch_size):\n",
    "                X_batch = X[i : i + self.batch_size]\n",
    "                y_batch = y[i : i + self.batch_size]\n",
    "\n",
    "                activations = self.__forward_propagate(X_batch)\n",
    "\n",
    "                y_prob = activations[-1]\n",
    "\n",
    "                accumulated_loss += self.__crossentropy_loss(y_batch, y_prob)\n",
    "                coef_grads = self.__back_propagation(activations, y_batch)\n",
    "\n",
    "                coef_grads = [grad / self.batch_size for grad in coef_grads]\n",
    "                accum_grad = [accum + grad**2 for accum, grad in zip(accum_grad, coef_grads)]\n",
    "                inv_accum_grad = [self.learning_rate / np.sqrt(self.eps + accum) for accum in accum_grad]\n",
    "                self.weights = [weight - inv_accum * grad for weight, inv_accum, grad in zip(self.weights, inv_accum_grad, coef_grads)]\n",
    "\n",
    "            if self.verbose:\n",
    "                loss = accumulated_loss / X.shape[0]\n",
    "                y_pred = self.predict(X_train)\n",
    "                accuracy = (y_pred == y_train).mean()\n",
    "                print(\"Epoch {}/{};\\t Train accuracy: {:.3f} \\t Loss : {:.3f}\".format(j + 1, self.max_iter, accuracy, loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self.__forward_propagate(X)\n",
    "        y_pred = activations[-1]\n",
    "        return self._label_binarizer.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (60000, 784) (60000,)\n",
      "test size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "data_train = pd.read_csv(\"../dataset/mldata/mnist_train.csv\", header=None)\n",
    "data_test = pd.read_csv(\"../dataset/mldata/mnist_test.csv\", header=None)\n",
    "\n",
    "x_train = np.ascontiguousarray(data_train[data_train.columns[:-1]].values, dtype=np.float32)\n",
    "y_train = np.ascontiguousarray(data_train[data_train.columns[-1]].values, dtype=np.float32)\n",
    "x_test = np.ascontiguousarray(data_test[data_test.columns[:-1]].values, dtype=np.float32)\n",
    "y_test = np.ascontiguousarray(data_test[data_test.columns[-1]].values, dtype=np.float32)\n",
    "\n",
    "print('train size: ', x_train.shape, y_train.shape)\n",
    "print('test size: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameters for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший подбор параметра для DNNClassifier: {'hidden_layer_sizes': (16,)}\n",
      "Лучший scope для DNNClassifier: 0.922\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'hidden_layer_sizes': [(16,), (32,), (48,), (64,), (76,), (92,)],\n",
    "}\n",
    "\n",
    "estimator = DNNClassifier(solver='adagrad',\n",
    "     batch_size=256, learning_rate=0.05, max_iter=50,\n",
    "     random_state=777, verbose=False)\n",
    "\n",
    "clf = GridSearchCV(estimator, parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Лучший подбор параметра для DNNClassifier: {}\".format(clf.best_params_))\n",
    "print(\"Лучший scope для DNNClassifier: {}\".format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f98e2abc6d8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAELCAYAAAAcKWtPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrmMeTBxzjmXsf8H9tPCdPkYVp4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_).plot(x='param_hidden_layer_sizes', y=[ 'mean_test_score', 'mean_train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(16,),\n",
       "       learning_rate=0.05, max_iter=30, momentum=0.9, random_state=777,\n",
       "       shuffle=True, solver=None, verbose=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = clf.best_estimator_\n",
    "up_params = {'verbose': True, 'max_iter': 30}\n",
    "best_estimator.set_params(**up_params)\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30;\t Train accuracy: 0.917 \t Loss : 0.405\n",
      "Epoch 2/30;\t Train accuracy: 0.922 \t Loss : 0.293\n",
      "Epoch 3/30;\t Train accuracy: 0.924 \t Loss : 0.279\n",
      "Epoch 4/30;\t Train accuracy: 0.927 \t Loss : 0.272\n",
      "Epoch 5/30;\t Train accuracy: 0.928 \t Loss : 0.268\n",
      "Epoch 6/30;\t Train accuracy: 0.930 \t Loss : 0.264\n",
      "Epoch 7/30;\t Train accuracy: 0.929 \t Loss : 0.262\n",
      "Epoch 8/30;\t Train accuracy: 0.930 \t Loss : 0.260\n",
      "Epoch 9/30;\t Train accuracy: 0.931 \t Loss : 0.258\n",
      "Epoch 10/30;\t Train accuracy: 0.929 \t Loss : 0.257\n",
      "Epoch 11/30;\t Train accuracy: 0.932 \t Loss : 0.255\n",
      "Epoch 12/30;\t Train accuracy: 0.932 \t Loss : 0.254\n",
      "Epoch 13/30;\t Train accuracy: 0.932 \t Loss : 0.253\n",
      "Epoch 14/30;\t Train accuracy: 0.933 \t Loss : 0.252\n",
      "Epoch 15/30;\t Train accuracy: 0.932 \t Loss : 0.251\n",
      "Epoch 16/30;\t Train accuracy: 0.933 \t Loss : 0.250\n",
      "Epoch 17/30;\t Train accuracy: 0.933 \t Loss : 0.249\n",
      "Epoch 18/30;\t Train accuracy: 0.934 \t Loss : 0.249\n",
      "Epoch 19/30;\t Train accuracy: 0.933 \t Loss : 0.248\n",
      "Epoch 20/30;\t Train accuracy: 0.934 \t Loss : 0.247\n",
      "Epoch 21/30;\t Train accuracy: 0.934 \t Loss : 0.247\n",
      "Epoch 22/30;\t Train accuracy: 0.934 \t Loss : 0.246\n",
      "Epoch 23/30;\t Train accuracy: 0.934 \t Loss : 0.246\n",
      "Epoch 24/30;\t Train accuracy: 0.933 \t Loss : 0.245\n",
      "Epoch 25/30;\t Train accuracy: 0.934 \t Loss : 0.245\n",
      "Epoch 26/30;\t Train accuracy: 0.934 \t Loss : 0.244\n",
      "Epoch 27/30;\t Train accuracy: 0.935 \t Loss : 0.244\n",
      "Epoch 28/30;\t Train accuracy: 0.934 \t Loss : 0.244\n",
      "Epoch 29/30;\t Train accuracy: 0.935 \t Loss : 0.243\n",
      "Epoch 30/30;\t Train accuracy: 0.935 \t Loss : 0.243\n",
      "CPU times: user 1min 53s, sys: 3min 46s, total: 5min 39s\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(16,),\n",
       "       learning_rate=0.05, max_iter=30, momentum=0.9, random_state=777,\n",
       "       shuffle=True, solver=None, verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "best_estimator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_estimator.predict(x_test)\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus the sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73436694\n",
      "Iteration 2, loss = 0.35062043\n",
      "Iteration 3, loss = 0.30739383\n",
      "Iteration 4, loss = 0.28902413\n",
      "Iteration 5, loss = 0.27904313\n",
      "Iteration 6, loss = 0.27252593\n",
      "Iteration 7, loss = 0.26722828\n",
      "Iteration 8, loss = 0.26373192\n",
      "Iteration 9, loss = 0.26013310\n",
      "Iteration 10, loss = 0.25787084\n",
      "Iteration 11, loss = 0.25518767\n",
      "Iteration 12, loss = 0.25345777\n",
      "Iteration 13, loss = 0.25129781\n",
      "Iteration 14, loss = 0.25017786\n",
      "Iteration 15, loss = 0.24874089\n",
      "Iteration 16, loss = 0.24708613\n",
      "Iteration 17, loss = 0.24613000\n",
      "Iteration 18, loss = 0.24500568\n",
      "Iteration 19, loss = 0.24473631\n",
      "Iteration 20, loss = 0.24350242\n",
      "Iteration 21, loss = 0.24221757\n",
      "Iteration 22, loss = 0.24145957\n",
      "Iteration 23, loss = 0.24186943\n",
      "Iteration 24, loss = 0.24089721\n",
      "Iteration 25, loss = 0.23973297\n",
      "Iteration 26, loss = 0.23929363\n",
      "Iteration 27, loss = 0.23851964\n",
      "Iteration 28, loss = 0.23824422\n",
      "Iteration 29, loss = 0.23767271\n",
      "Iteration 30, loss = 0.23757961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=0, batch_size=256, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(16,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=30, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=777,\n",
       "       shuffle=True, solver='adam', tol=0, validation_fraction=0.1,\n",
       "       verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,), max_iter=30, alpha=0, activation='identity',\n",
    "                     batch_size=256, solver='adam', verbose=10, random_state=777,\n",
    "                     tol=0, shuffle=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9267\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "print((y_pred == y_test).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
